DATA_PATH=/home/public_space/zhangxiaohong/public_user/PubChemSFT/train.pkl
IMAGE_PATH=/home/public_space/zhangxiaohong/public_user/PubChemSFT/

FINETUNE_TRAIN_RECIPE=common
TUNE_TYPE_LLM=frozen
OUTPUT_DIR=/home/public_space/zhangxiaohong/public_user/LLaVA-Steering/test-output
RUN_NAME=test-run

# training hyperparameters
NUM_TRAIN_EPOCHS=1
PER_DEVICE_TRAIN_BATCH_SIZE=32
PER_DEVICE_EVAL_BATCH_SIZE=4
GRADIENT_ACCUMULATION_STEPS=1
LEARNING_RATE=1e-3

# MORES_CONFIG_PATH=scripts/exp/mores_config/mores_config1.json
# INTERVENTION_POSITIONS=f4+l5
# MORES_SHARE_WEIGHTS=True
# INTERVENE_MODALITY=vis

CONV_VERSION=llama

LLM_VERSION=TinyLlama/TinyLlama-1.1B-Chat-v1.0
VT_VERSION="molecule_stm"
VT_VERSION2=""
CN_VERSION=mlp2x_gelu #connector type, other options are: qformer, resampler, etc
MODEL_MAX_LENGTH=2048

GIN_NUM_LAYERS=5
GIN_HIDDEN_DIM=300
GRAPH_DROP_RATIO=0.1
GRAPH_POOLING=mean
GRAPH_INIT_CHECKPOINT="/home/public_space/zhangxiaohong/public_user/LLaVA-Steering/molecule_model.pth"

bash scripts/train/finetune_phi2.sh "$DATA_PATH" "$IMAGE_PATH" "$VERSION" "$FINETUNE_TRAIN_RECIPE" "$TUNE_TYPE_LLM" "$OUTPUT_DIR" "$RUN_NAME" "$NUM_TRAIN_EPOCHS" "$PER_DEVICE_TRAIN_BATCH_SIZE" "$PER_DEVICE_EVAL_BATCH_SIZE" "$GRADIENT_ACCUMULATION_STEPS" "$LEARNING_RATE" "$MORES_CONFIG_PATH" "$INTERVENTION_POSITIONS" "$MORES_SHARE_WEIGHTS" "$INTERVENE_MODALITY" "$INTERVENTION_POSITIONS_2" "$MORES_SHARE_WEIGHTS_2" "$INTERVENE_MODALITY_2" "$TINYLLAVA_VERSION" "$PRETRAINED_MODEL_PATH" "$CONV_VERSION" "$LLM_VERSION" "$VT_VERSION" "$VT_VERSION2" "$CN_VERSION" "$MODEL_MAX_LENGTH" "$LORA_R" "$LORA_ALPHA" "$GIN_NUM_LAYERS" "$GIN_HIDDEN_DIM" "$GRAPH_DROP_RATIO" "$GRAPH_POOLING" "$GRAPH_INIT_CHECKPOINT"

