#!/bin/bash
deepspeed --include localhost:4,5,6,7 --master_port 25565 tinyllava/train/train.py \
    --deepspeed ./scripts/zero2.json \
    --data_path /home/public_space/zhangxiaohong/public_user/PubChemSFT/train.pkl \
    --output_dir /home/public_space/zhangxiaohong/public_user/LLaVA-Steering/test-vicuna-mores \
    --run_name test-vicuna-mores \
    --is_multimodal True \
    --mm_vision_select_layer -2 \
    --image_aspect_ratio square \
    --attn_implementation flash_attention_2 \
    --fp16 False \
    --bf16 True \
    --training_recipe mores \
    --tune_type_llm mores \
    --tune_type_connector full \
    --tune_type_vision_tower full \
    --tune_vision_tower_from_layer 0 \
    --conv_version llama \
    --model_name_or_path /home/public_space/zhangxiaohong/yintaoo/vicuna-7b \
    --vision_tower molecule_stm \
    --connector_type mlp2x_gelu \
    --gin_num_layers 5 \
    --gin_hidden_dim 300 \
    --graph_drop_ratio 0.1 \
    --graph_pooling mean \
    --graph_init_checkpoint /home/public_space/zhangxiaohong/public_user/LLaVA-Steering/molecule_model.pth \
    --mores_config_path scripts/exp/mores_config/mores_config1.json \
    --intervention_positions f4+l5 \
    --mores_share_weights True \
    --intervene_modality vis \
    --model_max_length 2048 \
    --group_by_modality_length False \
    --num_train_epochs 1 \
    --per_device_train_batch_size 32 \
    --per_device_eval_batch_size 4 \
    --gradient_accumulation_steps 1 \
    --evaluation_strategy no \
    --epoch_to_save 1 \
    --save_step 1000 \
    --learning_rate 8e-5 \
    --weight_decay 0. \
    --warmup_ratio 0.03 \
    --lr_scheduler_type cosine \
    --logging_steps 1 \
    --tf32 False \
    --gradient_checkpointing True \
    --dataloader_num_workers 32 \
    --lazy_preprocess True \
    --report_to tensorboard \
    --tokenizer_use_fast False
#    --lora_r 16 \
#    --lora_alpha 32 \
